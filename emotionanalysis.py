# -*- coding: utf-8 -*-
"""EmotionAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YAggpe9bwomWGKqaxRDh02Qy7HE3qy6A
"""



import pandas as pd

# Load the training dataset
try:
    df = pd.read_csv('/content/training.csv')
    print("Dataset loaded successfully!")
    print(df.head())
except FileNotFoundError:
    print("Error: training.csv not found. Please upload the file.")
except Exception as e:
    print(f"An error occurred: {e}")

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')


# Check for missing values
print("Missing values before handling:")
print(df.isnull().sum())

# Handle missing values (if any)
df.dropna(inplace=True)

# Text preprocessing function
def preprocess_text(text):
    text = text.lower()  # Lowercase text
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    tokens = word_tokenize(text)  # Tokenize text
    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords
    return ' '.join(tokens)

df['cleaned_text'] = df['text'].apply(preprocess_text)

print("\nMissing values after handling:")
print(df.isnull().sum())
print("\nData after preprocessing:")
print(df.head())

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You can adjust max_features

# Fit and transform the cleaned text data
tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_text'])

print("TF-IDF features shape:", tfidf_features.shape)

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['label'], test_size=0.2, random_state=42)

print("Training features shape:", X_train.shape)
print("Testing features shape:", X_test.shape)
print("Training labels shape:", y_train.shape)
print("Testing labels shape:", y_test.shape)

from sklearn.linear_model import LogisticRegression

# Initialize Logistic Regression model
model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence

# Train the model
model.fit(X_train, y_train)

print("Logistic Regression model trained successfully!")

from sklearn.metrics import classification_report

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

!pip install gradio -q

import gradio as gr
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Make sure you have downloaded these resources if running in a new environment
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')


# Text preprocessing function (same as before)
def preprocess_text(text):
    text = text.lower()  # Lowercase text
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    tokens = word_tokenize(text)  # Tokenize text
    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords
    return ' '.join(tokens)

# Assuming tfidf_vectorizer and model are already trained from previous steps

def predict_emotion(text):
    # Preprocess the input text
    cleaned_text = preprocess_text(text)
    # Transform the cleaned text using the trained TF-IDF vectorizer
    text_features = tfidf_vectorizer.transform([cleaned_text])
    # Predict the emotion label
    prediction = model.predict(text_features)
    # Map the predicted label back to emotion
    # You might need a dictionary to map labels to emotions based on your dataset
    # For now, let's just return the predicted label
    emotion_map = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}
    return emotion_map.get(prediction[0], 'unknown')


# Create the Gradio interface
iface = gr.Interface(
    fn=predict_emotion,
    inputs=gr.Textbox(lines=2, placeholder="Enter text here..."),
    outputs="text",
    title="Emotion Detection from Text",
    description="Enter a sentence to detect the emotion."
)

# Launch the interface
iface.launch(debug=True)